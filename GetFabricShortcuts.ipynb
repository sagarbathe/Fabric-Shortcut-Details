{"cells":[{"cell_type":"markdown","source":["\n","#### Run the cell below to install the required packages for Copilot\n"],"metadata":{"jupyter":{"magics_cell_name":"magics-cell-markdown","magics_signature":"27ac753c3c60167f65c4d05fa7809cd85f1f0273d5b842aca4f65a01"},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8924678-475d-419e-ad92-eefebc47ff78"},{"cell_type":"markdown","source":["**Please ensure all capacities in your Fabric tenant are running before running this script**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b56b345-8f39-4a6d-a77c-54cc60a50d6e"},{"cell_type":"code","source":["#Specify variables\n","myLakehouse = 'lakehouse03'\n","myWorkspace = 'WS_SagarFabric03'\n","myTablePath = \"abfss://\"+myWorkspace+\"@onelake.dfs.fabric.microsoft.com/\"+myLakehouse+\".Lakehouse/Tables/ShortcutsMetadata\"\n","#print(myTablePath)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb739cd6-4658-4dcc-aeda-7ebf17a36f39"},{"cell_type":"code","source":["pip install jsonmerge"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8df5afc-348b-412c-baf7-208ce68c49c8"},{"cell_type":"code","source":["import json, requests, pandas as pd, jsonmerge\n","import datetime\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"414e84a3-d523-402f-871c-2e81e6f9d2ed"},{"cell_type":"code","source":["# Generate token for Fabric access\n","access_token = mssparkutils.credentials.getToken('pbi')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9370376-6aa5-4095-b2b1-d8e5a324f1d2"},{"cell_type":"code","source":["# Function definitions to query Fabric REST API\n","\n","# List workspaces in current tenant\n","def listWorkspaces(access_token):\n","     base_url = 'https://api.fabric.microsoft.com/v1/admin/workspaces?state=Active&type=Workspace'\n","     header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","     response = requests.get(base_url, headers=header)\n","     data=response.json()\n","     jsondata = json.loads(json.dumps(response.text))\n","     #df = spark.read.json(sc.parallelize([jsondata]))\n","     return jsondata\n","\n","# List items of specific type inside of a workspace\n","def listWorkspaceItems(access_token,workspaceId,itemType):\n","     base_url = f\"https://api.fabric.microsoft.com/v1/admin/items?workspaceId={workspaceId}&type={itemType}\"\n","     header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","     response = requests.get(base_url, headers=header)   \n","     data=response.json()\n","     jsondata = json.loads(json.dumps(response.text))\n","     #df = spark.read.json(sc.parallelize([data]))\n","     return jsondata\n","    \n","# Retrieve shortcut information for individual shortcuts\n","def getShortcutInfo(access_token,workspaceId,lakehouseId,shortcutName):\n","     base_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{lakehouseId}/shortcuts/Tables/{shortcutName}\"\n","     header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","     response = requests.get(base_url, headers=header)\n","     data=response.json()\n","     return data\n","\n","# List all tables inside of a specific Lakehouse\n","def listLakeHouseTables(access_token,workspaceId,lakehouseId):\n","     base_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}/tables\"\n","     header = {\"Authorization\": f\"Bearer {access_token}\"}\n","     response = requests.get(base_url, headers=header)\n","     data=response.json()\n","     return data\n","\n","# List all file shortcuts inside of a specific Lakehouse\n","def listFileShortcuts(access_token,workspaceId,lakehouseId):\n","     base_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{lakehouseId}/shortcuts\"\n","     header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","     response = requests.get(base_url, headers=header)\n","     data=response.json()\n","     return data\n","\n","# List all shortcuts inside of a specific Lakehouse\n","def listallShortcuts(access_token,workspaceId,lakehouseId):\n","     base_url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{lakehouseId}/shortcuts\"\n","     header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","     response = requests.get(base_url, headers=header)\n","     data=response.json()\n","     jsondata = json.loads(json.dumps(response.text))\n","     #df = spark.read.json(sc.parallelize([data]))\n","     return jsondata"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86d66c9a-610f-4922-8c8d-cfac6841ecdf"},{"cell_type":"markdown","source":["**The cell below uses listWorkspaces, listWorkspaceItems and listallShortcuts functions defined above to get all shortcuts information. listWorkspaceItems gets all Lakehouses within a given workspace. listallShortcuts then iterates through all the lakehouses and gets all shortcuts within each lakehouse**\n","\n","**The reason the code below uses mergejson and not dataframe joins is because all shortcut details do not have the lakehouse/workspace id to join the 3 dataframes (workspace, lakehouse and shortcuts). Hence we have loop through each workspace, lakehouse and shortcut and append relevant content together**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2d47b8b-c43c-4072-b5da-880181a4cef4"},{"cell_type":"code","source":["from io import StringIO\n","\n","# define dataframe for all shortcuts details\n","df_sh_all = pd.DataFrame()\n","\n","#get all workspaces and iterate through them\n","df_ws = pd.read_json(StringIO(listWorkspaces(access_token)))\n","for index, row in df_ws.iterrows():\n","\n","    # Get the workspace details to add to the shortcuts info\n","    workspace_id = (row['workspaces']['id'])\n","    workspace_name = (row['workspaces']['name'])\n","    workspace_capacity_id = (row['workspaces']['capacityId'])\n","\n","    # Get all lakehouses in a workspace and iterate through them \n","    df_lh = pd.read_json(StringIO(listWorkspaceItems(access_token,workspace_id,\"Lakehouse\")))\n","\n","    # If no lakehouses in a workspace, create a record for the df_sh_all with no lakehouse and shortcut data\n","    if df_lh.empty:\n","        newrow = {'value':None,\n","                  'workspace_id':workspace_id,\n","                  'workspace_name':workspace_name,\n","                  'workspace_capacity_id':workspace_capacity_id,\n","                  'lakehouse_id':None,\n","                  'lakehouse_name':None\n","                 }\n","        #display(newrow)\n","        df_sh_all = df_sh_all._append(newrow, ignore_index=True)\n","    else:# iterate through lakehouses\n","\n","        for index, row in df_lh.iterrows():\n","\n","            # Get the lakehouse details to add to the shortcuts info\n","            lakehouse_id = (row['itemEntities']['id'])\n","            lakehouse_name = (row['itemEntities']['name'])\n","        \n","            newrow = {'workspace_id':workspace_id,\n","                      'workspace_name':workspace_name,\n","                      'workspace_capacity_id':workspace_capacity_id,\n","                      'lakehouse_id':lakehouse_id,\n","                      'lakehouse_name':lakehouse_name\n","                      }\n","            #display(newrow)      \n","\n","            # Get all shortcuts in a lakehouse and iterate through them \n","            df_sh = pd.read_json(StringIO(listallShortcuts(access_token,workspace_id,lakehouse_id)))\n","\n","            # If no shortcuts in the lakehouse, create a record for the df_sh_all with no shortcut data\n","            if df_sh.empty:\n","                \n","                newrow = {'value':None,\n","                          'workspace_id':workspace_id,\n","                          'workspace_name':workspace_name,\n","                          'workspace_capacity_id':workspace_capacity_id,\n","                          'lakehouse_id':lakehouse_id,\n","                         'lakehouse_name':lakehouse_name\n","                         }\n","                df_sh_all = df_sh_all._append(newrow, ignore_index=True)         \n","\n","            else:# iterate through shortcuts\n","            \n","                #for index, row in df_sh.iterrows(): \n","                for i in df_sh.index:\n","                    #display(row['value'])   \n","\n","                    # get all details for a shortcut, convert to json dict\n","                    js_sh = df_sh.iloc[i].to_json()\n","                    js_sh = json.loads(js_sh)\n","                    #display(js_sh)\n","                    #js_ws_lh = json.loads(newrow)\n","\n","                    # get details of workspace/lakehouse captured above, convert to json dict\n","                    js_ws_lh = json.loads(json.dumps(newrow))\n","                    #display(js_ws_lh)\n","\n","                    # merge the 2 json dicts\n","                    merg = jsonmerge.merge(js_sh, js_ws_lh)\n","                    #display(merg)\n","                    #df = pd.DataFrame(merg, index=[0])\n","                    #display(merg)\n","                    #mergej = json.dumps(merg)\n","                    #df_sh_temp = pd.read_json(StringIO(mergej))\n","\n","                    #append to the final dataframe with all content\n","                    df_sh_all = df_sh_all._append(merg, ignore_index=True)\n","                    \n","\n","    #print(\"===========================================\")   \n","\n","#display(df_sh_all)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d6e80199-ee45-4f16-a47f-1a22fa49d084"},{"cell_type":"code","source":["# Convert pandas dataframe to spark dataframe and load into a view for further processing\n","df_sh_all_spark = spark.createDataFrame(df_sh_all)\n","df_sh_all_spark.createOrReplaceTempView(\"vw_allShortcuts_01\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"694bde04-72af-47f5-88bf-c9efe91b4acc"},{"cell_type":"code","source":["%%sql\n","CREATE OR REPLACE TEMP VIEW vw_allShortcuts_02 AS\n","SELECT workspace_id, workspace_name, workspace_capacity_id, lakehouse_id, lakehouse_name, value.*\n","FROM vw_allShortcuts_01"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"765f9b3e-9e32-4ec9-bfce-d536bf955e91"},{"cell_type":"code","source":["%%sql\n","CREATE OR REPLACE TEMP VIEW vw_allShortcuts_03 AS\n","SELECT workspace_id, workspace_name, workspace_capacity_id, lakehouse_id, lakehouse_name,\n","       name AS shortcut_name, path AS shortcut_path, target.type AS shortcut_type,\n","       CASE WHEN target.type = 'OneLake'\n","            THEN target.oneLake.workspaceId\n","            ELSE NULL\n","       END AS target_workspace_id,\n","       to_json(target) AS shortcut_details\n","FROM vw_allShortcuts_02"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"eba357e2-85d5-4714-bc2e-9d0ee748da07"},{"cell_type":"code","source":["# Convert the workspaces pandas dataframe to spark dataframe and load into a view. This is for getting the name of the 'target' workspace which has the table/file which the shortcut points to\n","df_ws_spark = spark.createDataFrame(df_ws)\n","#display(df_sh_all_spark)\n","df_ws_spark.createOrReplaceTempView(\"vw_workspaces\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9449840-37f6-490e-959b-5d47cfc4481e"},{"cell_type":"code","source":["df_final = spark.sql(\"\"\"SELECT a.*, b.workspaces.name AS target_workspace_name, current_date AS LoadDatatime\n","                      FROM vw_allShortcuts_03 a\n","                      LEFT JOIN vw_workspaces b\n","                      ON a.target_workspace_id = b.workspaces.id\"\"\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"3d9d74ac-41ec-4e72-b94d-b96e40b9f565"},{"cell_type":"code","source":["#Drop and recreate the final table\n","if spark.catalog.tableExists(\"ShortcutsMetadata\"):\n","    drop_stmt = 'DROP TABLE '+myLakehouse+'.ShortcutsMetadata'\n","    result = spark.sql(drop_stmt)\n","    df_final.write.format(\"delta\").mode(\"overwrite\").save(myTablePath)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"520ed5f5-875e-4a21-8b14-81f79cd4b024"},{"cell_type":"markdown","source":["%%sql\n","select * from ShortcutsMetadata"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fb39a876-5662-47ce-beb6-f7ba1c3f8218"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[],"default_lakehouse":"5849f467-dd0a-4e9c-be00-b744441fe7c7","default_lakehouse_name":"lakehouse03","default_lakehouse_workspace_id":"0077781d-74c3-437e-b75c-42e640953c69"}}},"nbformat":4,"nbformat_minor":5}